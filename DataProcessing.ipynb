{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition's data was strongly anonymizer. Continuous features was transformed into categorical through histogram splitting. As a result each feature has no more than 30 values. We use this property in order to convert data into text files where each sample is a string. That format will allow us reshuffle data between neural net's epoches without high RAM load as unarchived data allocates a lot of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"nn_data\"):\n",
    "    shutil.rmtree(\"nn_data\")\n",
    "\n",
    "os.mkdir(\"nn_data\")\n",
    "os.mkdir(os.path.join(\"nn_data\", \"clear_test\"))\n",
    "os.mkdir(os.path.join(\"nn_data\", \"clear\"))\n",
    "os.mkdir(os.path.join(\"nn_data\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit2string(df, id2latters, feature_columns ):\n",
    "    return ' '.join((''.join((id2latters[a] for a in df[ft].tolist())) for ft in feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latters = list(string.ascii_uppercase)+list(string.ascii_lowercase)\n",
    "latters2id = {k:v for k,v in zip(latters, range(len(latters)))}\n",
    "id2latters = {v:k for k,v in latters2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = pd.read_parquet('./test_data/test_data_'+str(1)+'.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb4908a00c7458893f4ad5af899c840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO\n",
    "# write test data into train directory with dumb labels for further transductive autoencoder pretraining \n",
    "for chunk_i in tqdm(range(2)):\n",
    "\n",
    "    chunk = pd.read_parquet('./test_data/test_data_'+str(chunk_i)+'.pq')\n",
    "\n",
    "    ids = chunk[[\"id\"]].copy()\n",
    "    ids[\"counts\"] = 1\n",
    "    ids = ids.groupby(by=\"id\").sum().reset_index()\n",
    "    ids = ids.sort_values(by=\"id\")\n",
    "    labels = [' 2']*len(ids)\n",
    "\n",
    "    feature_columns = list(chunk.columns.values)\n",
    "    feature_columns.remove(\"id\")\n",
    "    feature_columns.remove(\"rn\")\n",
    "\n",
    "    series = chunk.groupby(\"id\").apply(lambda df:credit2string(df, id2latters, feature_columns ))\n",
    "    series = series.sort_index()\n",
    "    \n",
    "    st = '\\n'.join((p+s for p,s in zip(series, labels)))\n",
    "\n",
    "    fs = bz2.compress(bytearray(st, encoding='ascii'))\n",
    "\n",
    "    with open(os.path.join(\"nn_data\",\"clear_test\",\"chunk\"+str(chunk_i+12)+\".bz2\"), \"wb\") as f:\n",
    "        f.write(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('./train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140c65fd17fd4b399ee9abd0206a864e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert train data\n",
    "for chunk_i in tqdm(range(12)):\n",
    "\n",
    "    chunk = pd.read_parquet('./train_data/train_data_'+str(chunk_i)+'.pq')\n",
    "\n",
    "    ids = chunk[[\"id\"]].copy()\n",
    "    ids[\"counts\"] = 1\n",
    "    ids = ids.groupby(by=\"id\").sum().reset_index()\n",
    "    ids = ids.merge(y, on='id', how='left')\n",
    "    ids = ids.sort_values(by=\"id\")\n",
    "    labels = ids.flag.apply(lambda a: ' '+str(a)).tolist()\n",
    "\n",
    "    feature_columns = list(chunk.columns.values)\n",
    "    feature_columns.remove(\"id\")\n",
    "    feature_columns.remove(\"rn\")\n",
    "\n",
    "    series = chunk.groupby(\"id\").apply(lambda df:credit2string(df, id2latters, feature_columns ))\n",
    "    series = series.sort_index()\n",
    "    \n",
    "    st = '\\n'.join((p+s for p,s in zip(series, labels)))\n",
    "\n",
    "    fs = bz2.compress(bytearray(st, encoding='ascii'))\n",
    "\n",
    "    with open(os.path.join(\"nn_data\",\"clear\",\"chunk\"+str(chunk_i)+\".bz2\"), \"wb\") as f:\n",
    "        f.write(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b0b3c69a314a8f9a8d51cbb0a2e913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert test data\n",
    "for chunk_i in tqdm(range(2)):\n",
    "\n",
    "    chunk = pd.read_parquet('./test_data/test_data_'+str(chunk_i)+'.pq')\n",
    "\n",
    "    feature_columns = list(chunk.columns.values)\n",
    "    feature_columns.remove(\"id\")\n",
    "    feature_columns.remove(\"rn\")\n",
    "\n",
    "    series = chunk.groupby(\"id\").apply(lambda df:credit2string(df, id2latters, feature_columns ))\n",
    "    series = series.sort_index()\n",
    "    \n",
    "    st = '\\n'.join(series)\n",
    "    \n",
    "    fs = bz2.compress(bytearray(st, encoding='ascii'))\n",
    "\n",
    "    with open(os.path.join(\"nn_data\",\"test\",\"chunk\"+str(chunk_i)+\".bz2\"), \"wb\") as f:\n",
    "        f.write(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make categorical features to ids mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10be4326509a4a07ba86c6aecb804576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc29efd18204b0ba7f8e03957c65b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962995ff5caa462db0eb2ef1efdefb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global_mapping = {}\n",
    "\n",
    "MaxTr = -1\n",
    "for chunk_i in tqdm(range(12)):\n",
    "\n",
    "    chunk = pd.read_parquet('./train_data/train_data_'+str(chunk_i)+'.pq')\n",
    "    df=chunk\n",
    "    \n",
    "    feature_columns = list(df.columns.values)\n",
    "    feature_columns.remove(\"id\")\n",
    "    feature_columns.remove(\"rn\")\n",
    "    MaxTr = max(MaxTr, chunk.id.max())\n",
    "    \n",
    "    for ft in feature_columns:\n",
    "        if(ft in global_mapping):\n",
    "            global_mapping[ft] = global_mapping[ft] | (set(df[ft]))\n",
    "        else:\n",
    "            global_mapping[ft] = set(df[ft])\n",
    "\n",
    "for chunk_i in tqdm(range(2)):\n",
    "\n",
    "    chunk = pd.read_parquet('./test_data/test_data_'+str(chunk_i)+'.pq')\n",
    "    df=chunk\n",
    "    \n",
    "    feature_columns = list(df.columns.values)\n",
    "    feature_columns.remove(\"id\")\n",
    "    feature_columns.remove(\"rn\")\n",
    "    MaxTr = max(MaxTr, chunk.id.max())\n",
    "    \n",
    "    for ft in feature_columns:\n",
    "        if(ft in global_mapping):\n",
    "            global_mapping[ft] = global_mapping[ft] | (set(df[ft]))\n",
    "        else:\n",
    "            global_mapping[ft] = set(df[ft])\n",
    "\n",
    "for ft in tqdm(feature_columns):\n",
    "    mapping = {}\n",
    "    for k, v in zip(global_mapping[ft], range(len(global_mapping[ft]))):\n",
    "        mapping[k] = v\n",
    "    global_mapping[ft] = mapping.copy()\n",
    "    \n",
    "feat2id = {k:v for k, v in zip(feature_columns, range(len(feature_columns)))}\n",
    "id2feat = {v:k for k, v in zip(feature_columns, range(len(feature_columns)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feat2id.pickle', 'wb') as handle:\n",
    "    pickle.dump(feat2id, handle)\n",
    "\n",
    "with open('id2feat.pickle', 'wb') as handle:\n",
    "    pickle.dump(id2feat, handle)\n",
    "\n",
    "with open('global_mapping.pickle', 'wb') as handle:\n",
    "    pickle.dump(global_mapping, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
