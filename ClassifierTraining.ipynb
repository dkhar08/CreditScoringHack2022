{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import pickle\n",
    "import bz2\n",
    "import shutil\n",
    "from bz2 import BZ2Compressor\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from file_utils import saver_state, loader_state, shuffle, compressed_read, compressed_write, decompress\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_compression = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delte old data if there is. Recreate data directories.\n",
    "\n",
    "path = os.path.join(\"nn_data2\",\"train\")\n",
    "if os.path.isdir(path):\n",
    "    shutil.rmtree(path)\n",
    "os.mkdir(os.path.join(\"nn_data2\",\"train\"))\n",
    "\n",
    "path = os.path.join(\"nn_data2\",\"dev\")\n",
    "if os.path.isdir(path):\n",
    "    shutil.rmtree(path)\n",
    "os.mkdir(os.path.join(\"nn_data2\",\"dev\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy data from read only directory to work directory\n",
    "for i in range(12):\n",
    "    shutil.copy(os.path.join(\"nn_data2\",\"clear\",\"chunk\"+str(i)+\".bz2\"), os.path.join(\"nn_data2\",\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eccf4db3db940989296b3c1397afeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if not flag than do not perform compression due to speed reasons            \n",
    "if(do_compression == False):\n",
    "    decompress(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn_data2\\\\dev\\\\chunk11.bz2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(os.path.join(\"nn_data2\",\"train\",\"chunk\"+str(10)+\".bz2\"), os.path.join(\"nn_data2\",\"dev\"))\n",
    "shutil.copy(os.path.join(\"nn_data2\",\"train\",\"chunk\"+str(11)+\".bz2\"), os.path.join(\"nn_data2\",\"dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33deacf0e4d849dd80ac83cc7c8e758b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7915459c5894cd9a2229164d7d8d0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b32f94fec444e2a7a2426661386dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb0aa5dfda84ecc9202e480f5ab44f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ad96c314bb44f7ad1b43c479999642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make classifier validation\n",
    "with open('feat2id.pickle', 'rb') as handle:\n",
    "    feat2id = pickle.load(handle)\n",
    "\n",
    "with open('id2feat.pickle', 'rb') as handle:\n",
    "    id2feat = pickle.load(handle)\n",
    "\n",
    "with open('global_mapping.pickle', 'rb') as handle:\n",
    "    global_mapping = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_values = {}\n",
    "for ft in feat2id.keys():\n",
    "    MaxFeatureRange = len(global_mapping[ft])+1+1+1 #reserve space for special tokens\n",
    "    pad_values[ft] = MaxFeatureRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latters = list(string.ascii_uppercase)+list(string.ascii_lowercase)\n",
    "latters2id = {k:v for k,v in zip(latters, range(len(latters)))}\n",
    "id2latters = {v:k for k,v in latters2id.items()}\n",
    "\n",
    "#next example generator. Read files from chunk to chunk and yiel exaples from each chunk\n",
    "\n",
    "def get_next_example(part = \"train\", avaliable_chunks=(0,1,2,3,4,5,6,7,8,9, 10, 11)):\n",
    "    for chunk_i in avaliable_chunks:\n",
    "        with open(os.path.join(\"nn_data2\",part,\"chunk\"+str(chunk_i)+\".bz2\"), \"rb\") as f:\n",
    "            lines = compressed_read(f, do_compression = do_compression)\n",
    "        lines = lines.split('\\n')\n",
    "        for l in lines:\n",
    "            data = l.split(' ')\n",
    "            words = data[:-1]\n",
    "            y = data[-1]\n",
    "            \n",
    "            list_of_tokens_lists = [[latters2id[latter] for latter in word] for word in words]\n",
    "            \n",
    "            list_of_ids_lists = []\n",
    "            padding_shift = 3\n",
    "            \n",
    "            for tokl, nom in zip(list_of_tokens_lists, range(len(list_of_tokens_lists)) ):\n",
    "                list_of_ids_lists.append([global_mapping[id2feat[nom]][x] + padding_shift for x in tokl])\n",
    "            yield list_of_ids_lists + [int(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pos_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data convertion functions for classifier\n",
    "def process_feature(l, max_pos_len):\n",
    "    if(len(l) < max_pos_len):\n",
    "        l = [1] + l  + [0]*(max_pos_len - len(l))\n",
    "    elif(len(l) > max_pos_len):\n",
    "        l = [1] + l[-max_pos_len:] #[::-1]\n",
    "    else:\n",
    "        l = [1] + l  #[::-1]\n",
    "    return l\n",
    "\n",
    "#gather examples into batch\n",
    "def collate_fn(batch_list, max_pos_len):\n",
    "    feat_lists = [[] for _ in range(len(id2feat))]\n",
    "    y = []\n",
    "    lens = []\n",
    "    for example in batch_list:\n",
    "        for i in range(len(id2feat)):\n",
    "            l = example[i]\n",
    "            if(i==0):\n",
    "                lens.append(len(l) + 1 if(len(l) < max_pos_len) else max_pos_len+1)\n",
    "            l  = process_feature(l, max_pos_len)\n",
    "            feat_lists[i].append(l)\n",
    "        y.append(example[-1])\n",
    "    \n",
    "    return ([torch.tensor(x, dtype=torch.int32) for x in feat_lists], torch.tensor(y, dtype=torch.int64), lens)\n",
    "\n",
    "#generate batches for classifier\n",
    "def batch_gen(batch_size, max_pos_len = 50, part = \"train\", avaliable_chunks=(0,1,2,3,4,5,6,7,8,9, 10, 11)):\n",
    "    batch_list = []\n",
    "    i = 0\n",
    "    for x in get_next_example(part, avaliable_chunks):\n",
    "        batch_list.append(x)\n",
    "        i += 1\n",
    "        if(i == batch_size):\n",
    "            yield collate_fn(batch_list, max_pos_len)\n",
    "            batch_list = []\n",
    "            i = 0\n",
    "    if(len(batch_list)>0):\n",
    "        yield collate_fn(batch_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dev_classifier(model, nbatch):\n",
    "    targets = []\n",
    "    preds = []\n",
    "    ii = 0\n",
    "    for X, y, lens in tqdm(batch_gen(1000, max_pos_len, \"dev\", (10,11))):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = X[i].to(device)\n",
    "            y[i] = y[i].to(device)\n",
    "        tgt = y.numpy()\n",
    "        output = model(X, lens)[:,1] \n",
    "        pr = output.detach().cpu().numpy()\n",
    "        targets.append(tgt)\n",
    "        preds.append(np.exp(pr))\n",
    "        ii += 1\n",
    "        \n",
    "        if(ii>nbatch):\n",
    "            break\n",
    "    return roc_auc_score(np.hstack(targets), np.hstack(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(epoches_gone, experiment_name, device):\n",
    "    load_dir = os.path.join(experiment_name, \"lm_chekpoints\", str(epoches_gone), \"model.pt\")\n",
    "    model = loader_state(load_dir)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = ExponentialLR(optimizer, gamma = (1e-5/1e-3)**(1/(6000*7)))\n",
    "\n",
    "    loss_function = torch.nn.NLLLoss(weight=torch.tensor([1., 0.97/0.03]).to(device))#0.97/0.03\n",
    "    \n",
    "    logdir=os.path.join(experiment_name, \"classifier_logs\", str(epoches_gone))\n",
    "    \n",
    "    if os.path.isdir(logdir):\n",
    "        shutil.rmtree(logdir)\n",
    "\n",
    "    os.mkdir(logdir)\n",
    "\n",
    "    writer=SummaryWriter(log_dir=logdir)\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    epoches=3\n",
    "    ii = 0\n",
    "    for ep in range(epoches):\n",
    "        for X, y, lens in tqdm(batch_gen(500, max_pos_len)):\n",
    "            for i in range(len(X)):\n",
    "                X[i] = X[i].to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X, lens)\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            writer.add_scalar('Loss/train', loss.item() , ii)\n",
    "            if(ii%200 == 0):\n",
    "                print(loss.item())\n",
    "            if(ii%1000 == 0):\n",
    "                model.eval()\n",
    "                ev = eval_dev_classifier(model, 50)\n",
    "                print(\"after\",ii,\"steps short auc dev\", ev)\n",
    "                writer.add_scalar('auc dev short', ev , ii)\n",
    "                model.train()\n",
    "            ii+=1\n",
    "        #break\n",
    "        model.eval()\n",
    "        ev = eval_dev_classifier(model, 10**4)\n",
    "        print(\"after\",ep,\"epoches full auc dev\", ev)\n",
    "        writer.add_scalar('auc dev long', ev , ii)\n",
    "        model.train()\n",
    "        shuffle(\"train\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'with_test_nn2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load state of the pretrained encoder and run classifier training\n",
    "model = train_classifier(0, experiment_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latters = list(string.ascii_uppercase)+list(string.ascii_lowercase)\n",
    "latters2id = {k:v for k,v in zip(latters, range(len(latters)))}\n",
    "id2latters = {v:k for k,v in latters2id.items()}\n",
    "\n",
    "#test loader\n",
    "\n",
    "def get_next_example_test(part = \"test\", avaliable_chunks=(0,1)):\n",
    "    for chunk_i in avaliable_chunks:\n",
    "        with open(os.path.join(\"nn_data2\",part,\"chunk\"+str(chunk_i)+\".bz2\"), \"rb\") as f:\n",
    "            lines = compressed_read(f, do_compression = True)\n",
    "        lines = lines.split('\\n')\n",
    "        for l in lines:\n",
    "            data = l.split(' ')\n",
    "            words = data[:]\n",
    "            \n",
    "            list_of_tokens_lists = [[latters2id[latter] for latter in word] for word in words]\n",
    "            \n",
    "            list_of_ids_lists = []\n",
    "            padding_shift = 3\n",
    "            \n",
    "            for tokl, nom in zip(list_of_tokens_lists, range(len(list_of_tokens_lists)) ):\n",
    "                list_of_ids_lists.append([global_mapping[id2feat[nom]][x] + padding_shift for x in tokl])\n",
    "            yield list_of_ids_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature(l, max_pos_len):\n",
    "    if(len(l) < max_pos_len):\n",
    "        l = [1] + l  + [0]*(max_pos_len - len(l))\n",
    "    elif(len(l) > max_pos_len):\n",
    "        l = [1] + l[-max_pos_len:] #[::-1]\n",
    "    else:\n",
    "        l = [1] + l  #[::-1]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(batch_list, max_pos_len):\n",
    "    #max_pos_len = 50\n",
    "    feat_lists = [[] for _ in range(len(id2feat))]\n",
    "    y = []\n",
    "    lens = []\n",
    "    for example in batch_list:\n",
    "        for i in range(len(id2feat)):\n",
    "            l = example[i]\n",
    "            if(i==0):\n",
    "                lens.append(len(l) + 1 if(len(l) < max_pos_len) else max_pos_len+1)\n",
    "            \n",
    "            l  = process_feature(l, max_pos_len)\n",
    "            \n",
    "            feat_lists[i].append(l)\n",
    "    return ([torch.tensor(x, dtype=torch.int32) for x in feat_lists], lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen_test(batch_size, max_pos_len):\n",
    "    batch_list = []\n",
    "    i = 0\n",
    "    for x in get_next_example_test():\n",
    "        batch_list.append(x)\n",
    "        i += 1\n",
    "        if(i == batch_size):\n",
    "            yield collate_fn_test(batch_list, max_pos_len)\n",
    "            batch_list = []\n",
    "            i = 0\n",
    "    if(len(batch_list)>0):\n",
    "        yield collate_fn(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict test\n",
    "preds = []\n",
    "model.eval()\n",
    "for X, lens in tqdm(batch_gen_test(1000, max_pos_len)):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i].to(device)\n",
    "    output = model(X, lens)[:,1] \n",
    "    pr = output.detach().cpu().numpy()\n",
    "    preds.append(np.exp(pr))\n",
    "    \n",
    "preds = np.hstack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save preds\n",
    "df = pd.DataFrame({\n",
    "        \"id\": list(range(3000000, 3500000)),\n",
    "        \"score\": preds\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participants were offered to predict the debtor's default by his loan portfolio. Loan history was given for each bank's debtors. There is information about something around 5000000 debtors for the last 2 years. The competition's data was strongly anonymized. Continuous features were transformed into categorical through histogram splitting. As a result, each feature has no more than 30 values. Data have no explicit information about time, but debt information was sorted by time. The information in the test sample is in time after the information in the training sample.\n",
    "\n",
    "I was developing lgbm and lstm-based rnn models. In order to encode historical data to lgbm, I calculated the probability of meeting this loan in the portfolio of a bankrupt debtor for each loan. The meta-features were calculated by the out-of-fold stacking method. We can perceive these features as an assessment of how risky a loan is. That features combined with count-aggregated features yield lgbm's quality comparable with rnn models.\n",
    "\n",
    "Let's see how some count-aggregation features change their values over time.\n",
    "\n",
    "As we can see dataset endures a strong \"dataset shift\". There is a certain amount of resample and reweight classical methods to deal with that problem, but usually, they work only for weak models. No surprise, that they didn't work for my lgbm and nn models. Polynomial decaying weights somewhat improved lgbm's quality but not nn's.\n",
    "\n",
    "Further investigation showed that we could consider the \"Dataset shift\" problem as a special case of the domain adaptation problem. For nn models, there are a certain amount of advanced methods like ADDA and DANN. But usually, they work well for CNN, and they didn't work for me. For LSTM there are classical domain adaptation methods like LM and Autoencoder pretraining. Autoencoder pretraining showed a much stronger result than LM pretraining. And also its greatly outperformed lgbm with weight decay. As a result my final model was RNN-baseline with pretraining:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
